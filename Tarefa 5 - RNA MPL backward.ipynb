{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rede Neural Multicamadas (MPL) - Tatiane A. B. \nUma rede MPL é uma classe de rede neural artificial *feedforward* (ANN). Um MLP consiste em pelo menos três camadas de nós: uma camada de entrada , uma camada oculta e uma camada de saída . Exceto para os nós de entrada, cada nó é um neurônio que usa uma função de ativação não linear . O MLP utiliza uma técnica de aprendizado supervisionado chamada *backpropagation* para treinamento.\n\n\n\n### Implementando uma RNA multicamadas\n\nA imagem a seguir mostra a nossa rede, com as unidades de entrada marcadas como Input1, Input2 e Input3 (**Input Layer**) conectadas com os *nós* da camada oculta (**Hidden Layer**). Por sua vez as saída dos *nós* da camada oculda servem como entrada para os *nós*  da camada de saída (**Output Layer**). <img src='MPL.png' /><br>\n\n<p style=\"text-align:center\">  <i> Diagrama de uma MPL</i> </p>\n \n\nLembrando que em cada *nó* temos: \n\n$$f(h) = sigmoid(h)=\\frac 1 {1+e^{-h}}$$  \n\nonde\n\n<p style=\"text-align:center\"> $$h = \\frac 1n\\sum_{i=1}^n(w_i*x_i)+b$$  </p>\n\n\n\n\n    \n\n"},{"metadata":{},"cell_type":"markdown","source":"## Configuração da MPL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importando a biblioteca\nimport numpy as np\n\n#Função do cáculo da sigmóide\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n#Arquitetura da MPL\nN_input = 5\nN_hidden = 6\nN_output = 3\n\n#Vetor dos valores de entrada\nx = np.array([0.5, 0.1, -0.2])\ntarget =np.array([0.3, 0.8])\nlearnrate = 0.5\n\n#Pesos da Camada Oculta\nweights_in_hidden = np.array([[-0.07,  0.04, -0.05, 0.07],\n                              [ 0.04,  0.10,  0.02, 0.01],\n                              [-0.03,  0.04, -0.11, 0.06]])\n\n#Pesos da Camada de Saída\nweights_hidden_out = np.array([[-0.10,  0.09],\n                               [-0.04,  0.12],\n                               [-0.02,  0.04],\n                               [-0.01,  0.09]])\n\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forward Pass"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Camada oculta\n\n#Calcule a combinação linear de entradas e pesos sinápticos\nhidden_layer_input = np.dot(x, weights_in_hidden)\n\n#Aplicado a função de ativação\nhidden_layer_output = sigmoid(hidden_layer_input)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Camada de Saída\n\n#Calcule a combinação linear de entradas e pesos sinápticos\noutput_layer_in = np.dot(hidden_layer_output, weights_hidden_out)\n\n#Aplicado a função de ativação \noutput = sigmoid(output_layer_in)\n\nprint('As saídas da rede são',output)","execution_count":3,"outputs":[{"output_type":"stream","text":"As saídas da rede são [0.47885012 0.54255368]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Backward Pass"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## TODO: Cálculo do Erro\nerror = target - output\n#print('Erro da Rede: ',error)\n\n# TODO: Calcule o termo de erro de saída (Gradiente da Camada de Saída)\noutput_error_term = error * output * (1 - output)\n\n# TODO: Calcule a contribuição da camada oculta para o erro\nhidden_error = np.dot(weights_hidden_out,output_error_term)\n\n# TODO: Calcule o termo de erro da camada oculta (Gradiente da Camada Oculta)\nhidden_error_term = hidden_error * hidden_layer_output * (1 - hidden_layer_output)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Calcule a variação do peso da camada de saída\ndelta_w_h_o = learnrate * output_error_term*hidden_layer_output[:, None]\nprint('Variação do peso da camada de saída (delta_w_h_o): ',delta_w_h_o)","execution_count":5,"outputs":[{"output_type":"stream","text":"Variação do peso da camada de saída (delta_w_h_o):  [[-0.01101866  0.01577419]\n [-0.01128087  0.01614955]\n [-0.01115255  0.01596586]\n [-0.01129202  0.01616553]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Calcule a variação do peso da camada oculta\ndelta_w_i_h = learnrate * hidden_error_term * x[:, None]\nprint('Variação do peso da camada oculta (delta_w_i_h): ',delta_w_i_h)","execution_count":6,"outputs":[{"output_type":"stream","text":"Variação do peso da camada oculta (delta_w_i_h):  [[ 6.38265149e-04  5.90725285e-04  2.15529088e-04  3.87251147e-04]\n [ 1.27653030e-04  1.18145057e-04  4.31058176e-05  7.74502295e-05]\n [-2.55306060e-04 -2.36290114e-04 -8.62116351e-05 -1.54900459e-04]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Atualização dos Pesos"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Atualização dos Pesos\nweights_input_hidden = learnrate * delta_w_i_h\nprint('weights_input_hidden: ',weights_input_hidden)\nweights_hidden_output = learnrate * delta_w_h_o\nprint('weights_hidden_output: ',weights_hidden_output)","execution_count":7,"outputs":[{"output_type":"stream","text":"weights_input_hidden:  [[ 3.19132575e-04  2.95362642e-04  1.07764544e-04  1.93625574e-04]\n [ 6.38265149e-05  5.90725285e-05  2.15529088e-05  3.87251147e-05]\n [-1.27653030e-04 -1.18145057e-04 -4.31058176e-05 -7.74502295e-05]]\nweights_hidden_output:  [[-0.00550933  0.00788709]\n [-0.00564043  0.00807478]\n [-0.00557628  0.00798293]\n [-0.00564601  0.00808276]]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}